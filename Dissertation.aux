\relax 
\@writefile{toc}{\contentsline {chapter}{\noindent LIST OF TABLES}{vi}}
\citation{greff2016lstm}
\citation{reed2016generative}
\citation{ZX:14}
\citation{ZX14}
\citation{Comput.Sci.&Appl.Math.1997}
\citation{tsne}
\@writefile{toc}{\contentsline {chapter}{\noindent LIST OF FIGURES}{x}}
\@writefile{toc}{\contentsline {chapter}{\noindent ABSTRACT}{xi}}
\citation{KR:14}
\citation{SL:01}
\citation{MT:04}
\citation{JB:00}
\citation{FD:03}
\citation{HK:89}
\citation{HG:06}
\citation{LY:98}
\citation{LH:09}
\citation{RS:91}
\citation{KHM:83}
\citation{HG:68}
\@writefile{toc}{\noindent CHAPTER \vspace  {-10pt}}
\@writefile{toc}{\vspace *{3pt}}
\@writefile{toc}{\contentsline {chapter}{\makebox [0.75in][r]{1.}\hspace  *{3pt} \uppercase {INTRODUCTION}}{1}}
\@writefile{toc}{\vspace *{10pt}}
\newlabel{chapter: introduction}{{1}{1}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{1.1.} \hspace  *{3pt}Overview}{1}}
\newlabel{sec: overview}{{1.1}{1}}
\citation{AG:12}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{1.2.} \hspace  *{3pt}The Proposed Solutions}{2}}
\newlabel{sec: solution}{{1.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A screen-shot of street view point cloud data used in my research. There are 9195492 points that are contained in this scene.}}{3}}
\newlabel{fig: point cloud labelling}{{1.1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A demonstration of using synthetic template to simulate edge image of roofs used in my research.}}{4}}
\newlabel{fig: roof edge demonstration}{{1.2}{4}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{1.3.} \hspace  *{3pt}Novel Contribution}{4}}
\newlabel{sec: contribution}{{1.3}{4}}
\citation{Zhang2014Autoencoder}
\citation{AndiZang2015}
\citation{ZX:14}
\citation{Ouyang:17}
\citation{ZX17:OpticalFlow}
\citation{Zhang2014Autoencoder}
\citation{AndiZang2015}
\citation{ZX:14}
\citation{ZX:14b}
\citation{7424358}
\citation{ZX17:SMAE}
\citation{Zhang:2016:CCG:2983323.2983789}
\citation{torralba2011app_share}
\citation{remotesensing2013}
\citation{ZX:14}
\citation{Weiss}
\@writefile{toc}{\vspace *{3pt}}
\@writefile{toc}{\contentsline {chapter}{\makebox [0.75in][r]{2.}\hspace  *{3pt} \uppercase {Creation of Synthetic Data in Data Space}}{10}}
\@writefile{toc}{\vspace *{10pt}}
\newlabel{chapter: data space}{{2}{10}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{2.1.} \hspace  *{3pt}Introduction}{10}}
\newlabel{dataspace: introduction}{{2.1}{10}}
\citation{VT:03}
\citation{VT:04}
\citation{NJ:09}
\citation{BG:08}
\citation{CNV:02}
\citation{HH:05}
\citation{HH:08}
\citation{Pauly:2004:UVP:2386332.2386346}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{2.2.} \hspace  *{3pt}Related Work}{11}}
\newlabel{dataspace: relatedwork}{{2.2}{11}}
\citation{wood2016learning}
\citation{gupta2016synthetic}
\citation{wang2015deepfont}
\citation{gupta2014learning}
\citation{tompson2014real}
\citation{supancic2015depth}
\citation{ros2016synthia}
\citation{park2015articulated}
\citation{shakhnarovich2003fast}
\citation{lecun2004learning}
\citation{ionescu2014human3}
\citation{pishchulin2012articulated}
\citation{rogez2016mocap}
\citation{gaidon2016virtual}
\citation{goodfellow2014generative}
\citation{salimans2016improved}
\citation{radford2015unsupervised}
\citation{johnson2016perceptual}
\citation{odena2016conditional}
\citation{zhu2017unpaired}
\citation{lu2017conditional}
\citation{mansimov2015generating}
\citation{reed2016generative}
\citation{DBLP:journals/corr/DongZMG17}
\citation{zhang2016stackgan}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{2.3.} \hspace  *{3pt}A Brief Introduction of Proposed Methods}{14}}
\newlabel{dataspace: solution}{{2.3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Examples of (a) real roof edge vs. corresponding (b) synthetic roof edge images. The synthetic data is generated by the algorithms in Sec. 4. The examples are randomly drawn from SRC dataset.}}{15}}
\newlabel{fig: roof real and synI}{{2.1}{15}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{2.4.} \hspace  *{3pt}Creating Synthetic Data to Avoid Rarity Learning}{15}}
\citation{AndiZang2015}
\citation{Zhang2014Autoencoder}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  Footprint Segmentation}}{17}}
\newlabel{alg: 1}{{1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An example of input polygon vertices, cutting lines and six segmented components result.}}{18}}
\newlabel{fig: roofsegment}{{2.2}{18}}
\citation{ZQY:08}
\citation{otsu:79}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces An example of segmentation on a complex building roof. All pieces are normalize to a same dimension.}}{19}}
\newlabel{fig: segmentationexample}{{2.3}{19}}
\citation{Zhang2014Autoencoder}
\citation{ZX:14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Examples of roof top images used in my work.}}{20}}
\newlabel{fig: roofexamples}{{2.4}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The distribution of the roof styles used in the experiments.}}{20}}
\newlabel{Tab: datadistribution}{{2.1}{20}}
\citation{ZX:14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces For five roof styles used in my work, control points of each style are given in above figures which are shown as blue dots.}}{21}}
\newlabel{fig: controlpoints}{{2.5}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces For five roof styles used in my work, control points of each style are given in above figures which are shown as blue dots.}}{21}}
\newlabel{fig: controlpointsdistribution}{{2.6}{21}}
\citation{Zhang2014Autoencoder}
\citation{Zhang2014Autoencoder}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Get Matching Synthetic Image.}}{22}}
\newlabel{Alg: MatchingSynthetic}{{2}{22}}
\citation{Comput.Sci.&Appl.Math.1997}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  OptimizeControlPoints($U, V, \textbf  {S}$) Case 1}}{23}}
\newlabel{Alg: OCP1}{{3}{23}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces  RNNI($I, k, N$)}}{23}}
\newlabel{Alg: RNNI}{{4}{23}}
\citation{Bache+Lichman:2013}
\citation{ME:00}
\citation{GB:86}
\citation{lipovetsky2007efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Root images of all kinds of digit characters.}}{25}}
\newlabel{fig: rootimage}{{2.7}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Illustrations of the migration of control points and intermediate synthetic images generated using control points in each step. The distance transform images of the synthetic prototype and real images are shown as the left most and right most images respectively.}}{26}}
\newlabel{fig: migration1}{{2.8}{26}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces  OptimizeControlPoints($U, V, \textbf  {S}$) Case 2}}{26}}
\newlabel{Alg: OCP2}{{5}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Illustrations of the migration of control points for character 4. The control points are migrated from root image (blue) to destination image (red) and arrows are used to indicate points moving direction.}}{27}}
\newlabel{fig: migration2}{{2.9}{27}}
\citation{goodfellow2014generative}
\citation{arjovsky2017wasserstein}
\citation{greff2016lstm}
\citation{greff2016lstm}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Detailed schematic of a LSTM memory block as used in the hidden layers of a recurrent neural network. The figure is originally from \cite  {greff2016lstm}.}}{29}}
\newlabel{fig: lstm}{{2.10}{29}}
\citation{DBLP:journals/corr/DongZMG17}
\citation{reed2016generative}
\citation{zhang2016stackgan}
\citation{reed2016generative}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Network structure of the proposed text to image synthesis approach. The notation G represents generator and notation D represents discriminator.}}{31}}
\newlabel{fig: txt2img}{{2.11}{31}}
\citation{reed2016generative}
\citation{reed2016generative}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Comparison of image synthesis results using different input sentence features using network structure described in \cite  {reed2016generative}}}{32}}
\newlabel{fig: skipvec_vs_word2vec}{{2.12}{32}}
\citation{reed2016generative}
\citation{Nilsback08}
\citation{ZX:14}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{2.5.} \hspace  *{3pt}Creating Synthetic Data to Avoid Manual Labelling}{34}}
\citation{ZX:14}
\citation{ZX:14}
\citation{GG07}
\citation{7410673}
\citation{Ranjan_2017_CVPR}
\citation{7410673}
\citation{mayer2016large}
\citation{ahmadi2016unsupervised}
\citation{ren2017unsupervised}
\citation{DBLP:journals/corr/YuHD16}
\citation{NIPS2015_5854}
\citation{NIPS2015_5854}
\citation{ronneberger2015u}
\citation{Ilg_2017_CVPR}
\citation{NIPS2015_5854}
\newlabel{eqn: fuse optical flow}{{2.3}{39}}
\citation{rohlfing2003volume}
\citation{ashburner1999nonlinear}
\citation{Butler:ECCV:2012}
\citation{DFIB15}
\citation{DFIB15}
\citation{Geiger2012CVPR}
\citation{revaud2015epicflow}
\citation{weinzaepfel2013deepflow}
\citation{reed2016generative}
\citation{ren2017unsupervised}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Average endpoint errors (EPE) over occluded (OCC) and non-occluded areas (NOC) of our networks compard to peer methods and the proposed method on different datasets. }}{41}}
\newlabel{tab: optical flow comparison}{{2.2}{41}}
\citation{ren2017unsupervised}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Experiment setups to show predicted optical flows using the proposed approach could improve optical flow predict when combining with data with ground truth optical flow labels.}}{43}}
\newlabel{tab: optical flow experiments setup}{{2.3}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Experiment results.}}{44}}
\newlabel{tab: optical flow experiments train using predicted flows}{{2.4}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Examples of synthesized flower images. Please notice how each synthesized images correspond to works in description.}}{45}}
\newlabel{fig: txt2img results}{{2.13}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Illustration of all semantic points used in my work, red sphere on each roof represent the typical location of semantic point, the color bar under each image correspond to all color labels used in previous demo.}}{46}}
\newlabel{fig: All Semantic Points}{{2.14}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Roof styles I classified in \cite  {ZX:14}. From top to down, left to right: flat, shed, gable, hip, pyramid, curve, gambrel, mansard, hex and dome.}}{46}}
\newlabel{fig: allpointcloudroofstyles}{{2.15}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Illustration of erosion in my data. Three different levels of filtering are applied to each roof. In total, including original version, four versions are used in my work.}}{47}}
\newlabel{fig: All Semantic Points}{{2.16}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Illustration of all roof base models that are used in this work.}}{47}}
\newlabel{fig: baseroofs}{{2.17}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Illustration of network structure used in the proposed unsupervised optical flow learning neural network. Parts in red dash frame are novel contribution of my work.}}{48}}
\newlabel{fig: optical flow net struct}{{2.18}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces Results of image transformation using predicted optical flows. Images are grouped in three rows each. In each group, a pair of images inputted to network are in first row and third row. Results of using optical flow to transform first row images are given in second row.}}{49}}
\newlabel{fig: optical flow transform results}{{2.19}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Examples of pairs of images used as input of the proposed network and automatically generated masks are shown in this figure. }}{50}}
\newlabel{fig: optical flow masks}{{2.20}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces Comaprison of optical flows.}}{51}}
\newlabel{fig: optical flow comparison}{{2.21}{51}}
\citation{ZX14}
\citation{ZX:14}
\citation{ZX14}
\citation{ZX14}
\citation{Zhang2014Autoencoder}
\@writefile{toc}{\vspace *{3pt}}
\@writefile{toc}{\contentsline {chapter}{\makebox [0.75in][r]{3.}\hspace  *{3pt} \uppercase {Learning From Synthetic Data}}{52}}
\@writefile{toc}{\vspace *{10pt}}
\newlabel{chapter: learning from synthetic data}{{3}{52}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{3.1.} \hspace  *{3pt}Introduction}{52}}
\newlabel{Learn: introduction}{{3.1}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example of actual roof edge image and corresponding synthetic roof edge image in my work \cite  {ZX14} is shown in above figures.}}{52}}
\newlabel{fig: realandsynthetic}{{3.1}{52}}
\citation{Comput.Sci.&Appl.Math.1997}
\citation{Ulanova2014}
\citation{2012}
\citation{VT:03}
\citation{VT:04}
\citation{NJ:09}
\citation{Comput.Sci.&Appl.Math.1997}
\citation{2012}
\citation{Zhang2012}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{3.2.} \hspace  *{3pt}Related Work}{53}}
\newlabel{Learn: relatedwork}{{3.2}{53}}
\citation{Comput.Sci.&Appl.Math.1997}
\citation{Ulanova2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The schema of perturbation-based recognition of \cite  {Comput.Sci.&Appl.Math.1997} is shown above.}}{54}}
\newlabel{fig: inverseperturbation}{{3.2}{54}}
\citation{AndiZang2015}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{3.3.} \hspace  *{3pt}Features to Ignore Additional Information in Actual Data}{55}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Precision and recall of hip (HIP), gable (GBL), flat (FLT) and half hip (HHP) styles classified by Random Forest is shown in above table. I use red font to show highest value among results.}}{56}}
\newlabel{table: pr}{{3.1}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Illustration of HOR feature.}}{56}}
\newlabel{fig: HOR}{{3.3}{56}}
\citation{ZX14}
\citation{ZX14}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces  Histogram of Rays}}{57}}
\newlabel{alg: 2}{{6}{57}}
\citation{HB:77}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces The comparison of accuracies between HoG (HOG), Shape Context (SC), HoR (HOR) and their combinations by running Random Forest approach.}}{58}}
\newlabel{table: ac}{{3.2}{58}}
\citation{GB:88}
\citation{QZ:09}
\citation{JY:95}
\citation{DG:98}
\citation{DG:99}
\citation{JS:08}
\citation{ML:10}
\citation{AT:03}
\citation{AR:66}
\citation{AR:66}
\citation{GB:86}
\citation{UM:68}
\citation{PD:80}
\citation{PF:04}
\newlabel{Equ: Dist between U and V}{{3.2}{60}}
\newlabel{Equ: Dist between U and V 2}{{3.3}{60}}
\citation{ML:10}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Computing edge orientation in the binary template image.}}{61}}
\newlabel{fig: edge direction}{{3.4}{61}}
\citation{PF:04}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Example of the modified distance measure. In both cases I use a neighborhood of size $p=13$, and select the lowest $q=5$ neighbors. While the distance at the pixel is the same (1.5), the modified distance measure in the bottom example is higher due to the larger distance to neighbors.}}{62}}
\newlabel{fig: modified distance.}{{3.5}{62}}
\newlabel{Equ: distance metric}{{3.4}{62}}
\newlabel{equ: distance variance}{{3.5}{63}}
\citation{ML:10}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Results of matching partially occluded buildings. (a) Edge images detected from target image. (b)-(d) Results of the CM algorithm, the results of the DCM algorithm and the results of the proposed algorithm, respectively. (e) The pixels selected for computing the average error during the matching process.}}{65}}
\newlabel{fig: incomplete building results}{{3.6}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Experimental evaluation results on the San Francisco (upper row) and Chicago (lower row) datasets. The left and right columns show different evaluation metrics.}}{66}}
\newlabel{fig: result comparison}{{3.7}{66}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Accuracy results.}}{67}}
\newlabel{table: building in shadow}{{3.3}{67}}
\citation{ZX:14}
\citation{ZX:14}
\citation{RBR08}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Results of alignment using global constraint. The number on the right side of the arrows show the results obtained by alignment using the global constraint.}}{68}}
\newlabel{table: gc}{{3.4}{68}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{3.4.} \hspace  *{3pt}Features to Compensate Additional Information}{68}}
\citation{RO02}
\citation{RO02}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Illustration of the frame I used in the computation of the shape distribution features.}}{69}}
\newlabel{fig: ShapeDistribution}{{3.8}{69}}
\citation{JA:99}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Example of applying Gaussian smoothing to the spin image features. (a) The red dot shows the location where spin image features are computed. (b) Generated spin image without smoothing. (c) Result of spin image after random disruption.}}{70}}
\newlabel{fig: Gaussian Smooth}{{3.9}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Illustration of distribution of bumpiness at points with different slope.}}{71}}
\newlabel{fig: BumpinessDistribution}{{3.10}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Illustration of the histogram $H$ and the quadratic function fitted on it.}}{72}}
\newlabel{fig: BumpinessFlatness}{{3.11}{72}}
\citation{NVC02}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Point classification results obtained by running my point type classifier on dataset one (top row) and the dataset two (bottom row). The colors of points corresponds to the color bars of the point in Figure 2.16\hbox {}.}}{73}}
\newlabel{fig: More Point Type Results}{{3.12}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces The distribution of the roof styles in the two datasets.}}{74}}
\newlabel{fig: Roof Distribution}{{3.13}{74}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Precision and recall of Gaussian Mixture Model(GMM), K-Means(KM) and my approach are shown in above table. I use red font to show highest value among results obtained by these three approaches.}}{75}}
\newlabel{table: pr}{{3.5}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces The comparison of F-Score on the two datasets by running KM, GMM and the proposed approach.}}{76}}
\newlabel{fig: F-Score}{{3.14}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The confusion matrix of obtained by the proposed approach on two datasets.}}{77}}
\newlabel{fig: Confusion Matrix}{{3.15}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces The F-Score of each roof style obtained by using an increasing proportion of the training data.}}{77}}
\newlabel{fig: Increasing Training Set}{{3.16}{77}}
\@writefile{toc}{\vspace *{3pt}}
\@writefile{toc}{\contentsline {chapter}{\makebox [0.75in][r]{4.}\hspace  *{3pt} \uppercase {Elimination of Synthetic Gap}}{78}}
\@writefile{toc}{\vspace *{10pt}}
\newlabel{chapter: synthetic gap}{{4}{78}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{4.1.} \hspace  *{3pt}Introduction}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces t-SNE visulization of synthetic gap using the data from SRC dataset. (a) synthetic gap of real and synthetic data; (b) MCAE bridges the synthetic gap.}}{78}}
\newlabel{fig: tsne_vis}{{4.1}{78}}
\citation{borgwardt2006integrating}
\citation{huang2006correcting}
\citation{gong2013connecting}
\citation{pan2011domain}
\citation{gopalan2011domain}
\citation{baktashmotlagh2013unsupervised}
\citation{borgwardt2006integrating}
\citation{huang2006correcting}
\citation{gong2012geodesic}
\citation{fernando2013unsupervised}
\citation{gopalan2011domain}
\citation{gong2012geodesic}
\citation{chopra2013dlid}
\citation{glorot2011domain}
\citation{chopra2013dlid}
\citation{zeiler2014visualizing}
\citation{oquab2014learning}
\citation{babenko2014neural}
\citation{tzeng2014deep}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{4.2.} \hspace  *{3pt}Related Work}{79}}
\citation{vincent2008ICML}
\citation{MarginalizedDenoisingAutoencoders2012ICML}
\citation{Glorot11domainadaptation}
\citation{BP:12}
\citation{BY:12}
\citation{DJ:13}
\citation{Blitzer07Biographies}
\citation{Sarinnapakorn:2007:CST:1313047.1313197}
\citation{lampert13AwAPAMI}
\citation{lampert2009zeroshot_dat}
\citation{yanweiPAMIlatentattrib}
\citation{yao2011action_part}
\citation{yanweiembedding}
\citation{yanweiBMVC}
\citation{RohrbachCVPR12}
\citation{rohrbach2010semantic_transfer}
\citation{RichardNIPS13}
\citation{pan2009transfer_survey}
\citation{Ben-David:2010:TLD:1745449.1745461}
\citation{Weinberger:2009:FHL:1553374.1553516}
\citation{VP:10}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{4.3.} \hspace  *{3pt}Multichannel Autoencoder (MCAE)}{81}}
\newlabel{eq:encoder}{{4.1}{82}}
\newlabel{eq:decoding}{{4.2}{82}}
\newlabel{eq:reconstruction_error_basic}{{4.3}{82}}
\newlabel{Equ: SparsityObjective}{{4.4}{83}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces (a) Illustration of the proposed MCAE model in a stacked autoencoder structure, where black edge between two layers are linked to and shared by two tasks, red and blue links are separately connected to left and right task respectively. (b) A zoom in structure of MCAE. }}{83}}
\newlabel{fig: YAE-structure}{{4.2}{83}}
\newlabel{Equ: YAE-objective}{{4.5}{84}}
\newlabel{Equ: YAE-objective}{{4.7}{85}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{4.4.} \hspace  *{3pt}Evaluation}{86}}
\citation{LY:98}
\citation{multitask_autoencoder2011ICML}
\citation{VP:10}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces F1-score of roof style classification using reconstructed images (in CNN) and encoded image features (in SVM). Second column shows the data used to train the autoencoder in the first column. In classification, Real+\textit  {Syn II} are used in the training of CNN and SVM. ; $\textit  {Syn I}+\text  {Real}$ means that I use concatenation of the Syn I and real images as the input for the corresponding autoencoders.}}{88}}
\newlabel{Tab: RoofBetterAE}{{4.1}{88}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces F1-score of handwritten digit recognition.}}{88}}
\newlabel{Tab: DigitBetterAE}{{4.2}{88}}
\citation{tsne}
\citation{tsne}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Examples of images before and after being reconstructed by MCAE. It could be observed from the images that actual images and synthetic images look much more similar after being processed by MCAE.}}{89}}
\newlabel{fig: resultsofmacereconst}{{4.3}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Correlation between real and corresponding best matching \textit  {Syn I} data.}}{89}}
\newlabel{fig: correlation}{{4.4}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces t-SNE \cite  {tsne} visualization of synthetic gap bridged by MCAE. (a) Data distributions of each class of SRC dataset. For many data instances, the (circle) real and (dot points) synthetic data are not overlapping. This is synthetic gap. (b) Data distributions of the reconstructed images by MCAE for each class of SRC dataset. The reconstructed images of all the real (circle) and synthetic (dot points) are almost overlapped. It means that my MCAE can bridge the synthetic gap.}}{90}}
\newlabel{fig:t-SNE-visualization-of}{{4.5}{90}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces F1-score of roof style classification by classifier (CNN and SVM) using different set of data reconstructed of encoded using the proposed MCAE.}}{91}}
\newlabel{Tab: RoofBetterData}{{4.3}{91}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces F1-score of handwritten digit recognition.}}{91}}
\newlabel{Tab: DigitBetterData}{{4.4}{91}}
\citation{HH:09}
\citation{weiss2004mining}
\@writefile{toc}{\vspace *{3pt}}
\@writefile{toc}{\contentsline {chapter}{\makebox [0.75in][r]{5.}\hspace  *{3pt} \uppercase {Creation of Synthetic Data in Feature Space}}{92}}
\@writefile{toc}{\vspace *{10pt}}
\newlabel{chapter: feature space}{{5}{92}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{5.1.} \hspace  *{3pt}Introduction}{92}}
\citation{chawla2004editorial}
\citation{HH:09}
\citation{liu2009exploratory}
\citation{ZJMI03}
\citation{Drummond03c4.5}
\citation{CNV:02}
\citation{chawla2003smoteboost}
\citation{guo2004learning}
\citation{chen2010ramoboost}
\citation{HH:05}
\citation{HH:08}
\citation{barua2011novel}
\citation{barua2014mwmote}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{5.2.} \hspace  *{3pt}Problem Formulation}{95}}
\citation{2013Sharma}
\citation{elgammal2002background}
\citation{zhang2006bayesian}
\citation{CNV:02}
\newlabel{eqn: relative diff}{{5.5}{98}}
\newlabel{eqn: weight}{{5.6}{98}}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{5.3.} \hspace  *{3pt}Theoretical Guarantee Over SMOTE}{99}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Demonstration of CGMOS. In first two figures, diamonds represent minority samples and circles represent majority samples. The positions of synthesized data points are labeled using a star symbol on a horizontal line passing through the center. The x and y axes represent features. In the bottom figure the x axis indicates a location where a sample was added (in correspondence with the first two figures) whereas the y-axis indicates the relative certainty change.}}{100}}
\newlabel{fig: demoofdiffinsertion}{{5.1}{100}}
\newlabel{eqn. likelihoodratio}{{5.7}{101}}
\citation{Lichman:2013}
\citation{HH:09}
\citation{CNV:02}
\citation{HH:05}
\citation{HH:08}
\citation{barua2014mwmote}
\citation{chen2010ramoboost}
\citation{chawla2004editorial}
\citation{chawla2003smoteboost}
\citation{guo2004learning}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Summary of the datasets used in our experiments, where S\#, F\#, and R stand for the number of samples, the number of features, and imbalance ratio (defined as \#minority$/$\#majority).}}{103}}
\newlabel{tab: realdata}{{5.1}{103}}
\citation{Quinlan:1993}
\citation{fy:1996}
\citation{CNV:02}
\citation{HH:05}
\citation{HH:08}
\citation{barua2014mwmote}
\citation{chen2010ramoboost}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces A summary of performance measures for the majority and minority classes for all compared methods and artificial datasets.}}{105}}
\newlabel{tab: results_synthetic_data}{{5.2}{105}}
\citation{HH:09}
\citation{sokolova2006beyond}
\citation{fawcett2004roc}
\citation{fawcett2006introduction}
\citation{mohri2005confidence}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces A summary of AUC, $Precision$, $Recall$, $\mathop {F\mhyphen score}$ and $\mathop {G\mhyphen score}$ of all competitors for the majority and minority classes produced by 6 classifiers on the artificial datasets.}}{108}}
\newlabel{tab: results_real_data}{{5.3}{108}}
\citation{batista2004study}
\citation{weiss2003learning}
\citation{chawla2004editorial}
\citation{JLD:11}
\citation{SM:06}
\citation{demvsar2006statistical}
\citation{chen2010ramoboost}
\citation{barua2014mwmote}
\citation{GC09}
\citation{BF:08}
\citation{zimmerman1997teacher}
\citation{demvsar2006statistical}
\@writefile{toc}{\vspace {-10pt}}
\@writefile{toc}{\contentsline {section}{\makebox [0.3in][r]{5.4.} \hspace  *{3pt}Conclusion}{113}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces ROC curves of the artificial datasets classification results using b-kde classifier. In total, results of 6 datasets are shown in the above figures.}}{114}}
\newlabel{fig: synthetic roc}{{5.2}{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces ROC curves of classification results. From left to right, up to down, we show the results of 6 different classifiers: b-kde, knn, svm, nn, rf and Adaboost.M1. Curves in blue are the results of the proposed CGMOS.}}{115}}
\newlabel{fig: roc}{{5.3}{115}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparison of results when increasing the number of data synthesized for the minority class. The curves measure the average AUC of the ROC curves. Curves in blue are the results of the proposed CGMOS.}}{116}}
\newlabel{fig: increasingnum}{{5.4}{116}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces A summary of AUC of 8 oversampling algorithms over all 30 datasets used in our evaluation. The AUC is averaged over all 6 base classifiers used in the evaluation. It could be seen from above table that CGMOS achieves best AUC measures for 24 datasets out of 30. By average, the AUC of CGMOS is at least 2 percent higher than all other competitors.}}{117}}
\newlabel{tab: results_real_data_per_file}{{5.4}{117}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces A summary of $p$-values of statistical significant tests of classification results using CGMOS against each of all the other competitors. }}{118}}
\newlabel{tab: signrank}{{5.5}{118}}
\@writefile{toc}{\vspace *{3pt}}
\@writefile{toc}{\contentsline {chapter}{\makebox [0.75in][r]{6.}\hspace  *{3pt} \uppercase {Conclusion}}{119}}
\@writefile{toc}{\vspace *{10pt}}
\newlabel{chapter: conclusion}{{6}{119}}
\bibstyle{IEEEtran}
\bibdata{mybib}
\bibcite{greff2016lstm}{1}
\bibcite{reed2016generative}{2}
\bibcite{ZX:14}{3}
\bibcite{ZX14}{4}
\bibcite{Comput.Sci.&Appl.Math.1997}{5}
\bibcite{tsne}{6}
\bibcite{KR:14}{7}
\bibcite{SL:01}{8}
\bibcite{MT:04}{9}
\bibcite{JB:00}{10}
\bibcite{FD:03}{11}
\bibcite{HK:89}{12}
\bibcite{HG:06}{13}
\bibcite{LY:98}{14}
\bibcite{LH:09}{15}
\bibcite{RS:91}{16}
\@writefile{toc}{\vspace *{0pt}}
\@writefile{toc}{\contentsline {chapter}{\noindent BIBLIOGRAPHY}{122}}
\bibcite{KHM:83}{17}
\bibcite{HG:68}{18}
\bibcite{AG:12}{19}
\bibcite{Zhang2014Autoencoder}{20}
\bibcite{AndiZang2015}{21}
\bibcite{Ouyang:17}{22}
\bibcite{ZX17:OpticalFlow}{23}
\bibcite{ZX:14b}{24}
\bibcite{7424358}{25}
\bibcite{ZX17:SMAE}{26}
\bibcite{Zhang:2016:CCG:2983323.2983789}{27}
\bibcite{torralba2011app_share}{28}
\bibcite{remotesensing2013}{29}
\bibcite{Weiss}{30}
\bibcite{VT:03}{31}
\bibcite{VT:04}{32}
\bibcite{NJ:09}{33}
\bibcite{BG:08}{34}
\bibcite{CNV:02}{35}
\bibcite{HH:05}{36}
\bibcite{HH:08}{37}
\bibcite{Pauly:2004:UVP:2386332.2386346}{38}
\bibcite{wood2016learning}{39}
\bibcite{gupta2016synthetic}{40}
\bibcite{wang2015deepfont}{41}
\bibcite{gupta2014learning}{42}
\bibcite{tompson2014real}{43}
\bibcite{supancic2015depth}{44}
\bibcite{ros2016synthia}{45}
\bibcite{park2015articulated}{46}
\bibcite{shakhnarovich2003fast}{47}
\bibcite{lecun2004learning}{48}
\bibcite{ionescu2014human3}{49}
\bibcite{pishchulin2012articulated}{50}
\bibcite{rogez2016mocap}{51}
\bibcite{gaidon2016virtual}{52}
\bibcite{goodfellow2014generative}{53}
\bibcite{salimans2016improved}{54}
\bibcite{radford2015unsupervised}{55}
\bibcite{johnson2016perceptual}{56}
\bibcite{odena2016conditional}{57}
\bibcite{zhu2017unpaired}{58}
\bibcite{lu2017conditional}{59}
\bibcite{mansimov2015generating}{60}
\bibcite{DBLP:journals/corr/DongZMG17}{61}
\bibcite{zhang2016stackgan}{62}
\bibcite{ZQY:08}{63}
\bibcite{otsu:79}{64}
\bibcite{Bache+Lichman:2013}{65}
\bibcite{ME:00}{66}
\bibcite{GB:86}{67}
\bibcite{lipovetsky2007efficient}{68}
\bibcite{arjovsky2017wasserstein}{69}
\bibcite{mikolov2013efficient}{70}
\bibcite{Nilsback08}{71}
\bibcite{GG07}{72}
\bibcite{7410673}{73}
\bibcite{Ranjan_2017_CVPR}{74}
\bibcite{mayer2016large}{75}
\bibcite{ahmadi2016unsupervised}{76}
\bibcite{ren2017unsupervised}{77}
\bibcite{DBLP:journals/corr/YuHD16}{78}
\bibcite{NIPS2015_5854}{79}
\bibcite{ronneberger2015u}{80}
\bibcite{Ilg_2017_CVPR}{81}
\bibcite{rohlfing2003volume}{82}
\bibcite{ashburner1999nonlinear}{83}
\bibcite{Butler:ECCV:2012}{84}
\bibcite{DFIB15}{85}
\bibcite{Geiger2012CVPR}{86}
\bibcite{revaud2015epicflow}{87}
\bibcite{weinzaepfel2013deepflow}{88}
\bibcite{Ulanova2014}{89}
\bibcite{2012}{90}
\bibcite{Zhang2012}{91}
\bibcite{HB:77}{92}
\bibcite{GB:88}{93}
\bibcite{QZ:09}{94}
\bibcite{JY:95}{95}
\bibcite{DG:98}{96}
\bibcite{DG:99}{97}
\bibcite{JS:08}{98}
\bibcite{ML:10}{99}
\bibcite{AT:03}{100}
\bibcite{AR:66}{101}
\bibcite{UM:68}{102}
\bibcite{PD:80}{103}
\bibcite{PF:04}{104}
\bibcite{RBR08}{105}
\bibcite{RO02}{106}
\bibcite{JA:99}{107}
\bibcite{NVC02}{108}
\bibcite{borgwardt2006integrating}{109}
\bibcite{huang2006correcting}{110}
\bibcite{gong2013connecting}{111}
\bibcite{pan2011domain}{112}
\bibcite{gopalan2011domain}{113}
\bibcite{baktashmotlagh2013unsupervised}{114}
\bibcite{gong2012geodesic}{115}
\bibcite{fernando2013unsupervised}{116}
\bibcite{chopra2013dlid}{117}
\bibcite{glorot2011domain}{118}
\bibcite{zeiler2014visualizing}{119}
\bibcite{oquab2014learning}{120}
\bibcite{babenko2014neural}{121}
\bibcite{tzeng2014deep}{122}
\bibcite{vincent2008ICML}{123}
\bibcite{MarginalizedDenoisingAutoencoders2012ICML}{124}
\bibcite{Glorot11domainadaptation}{125}
\bibcite{BP:12}{126}
\bibcite{BY:12}{127}
\bibcite{DJ:13}{128}
\bibcite{Blitzer07Biographies}{129}
\bibcite{Sarinnapakorn:2007:CST:1313047.1313197}{130}
\bibcite{lampert13AwAPAMI}{131}
\bibcite{lampert2009zeroshot_dat}{132}
\bibcite{yanweiPAMIlatentattrib}{133}
\bibcite{yao2011action_part}{134}
\bibcite{yanweiembedding}{135}
\bibcite{yanweiBMVC}{136}
\bibcite{RohrbachCVPR12}{137}
\bibcite{rohrbach2010semantic_transfer}{138}
\bibcite{RichardNIPS13}{139}
\bibcite{pan2009transfer_survey}{140}
\bibcite{Ben-David:2010:TLD:1745449.1745461}{141}
\bibcite{Weinberger:2009:FHL:1553374.1553516}{142}
\bibcite{VP:10}{143}
\bibcite{multitask_autoencoder2011ICML}{144}
\bibcite{HH:09}{145}
\bibcite{weiss2004mining}{146}
\bibcite{chawla2004editorial}{147}
\bibcite{liu2009exploratory}{148}
\bibcite{ZJMI03}{149}
\bibcite{Drummond03c4.5}{150}
\bibcite{chawla2003smoteboost}{151}
\bibcite{guo2004learning}{152}
\bibcite{chen2010ramoboost}{153}
\bibcite{barua2011novel}{154}
\bibcite{barua2014mwmote}{155}
\bibcite{2013Sharma}{156}
\bibcite{elgammal2002background}{157}
\bibcite{zhang2006bayesian}{158}
\bibcite{Lichman:2013}{159}
\bibcite{Quinlan:1993}{160}
\bibcite{fy:1996}{161}
\bibcite{sokolova2006beyond}{162}
\bibcite{fawcett2004roc}{163}
\bibcite{fawcett2006introduction}{164}
\bibcite{mohri2005confidence}{165}
\bibcite{batista2004study}{166}
\bibcite{weiss2003learning}{167}
\bibcite{JLD:11}{168}
\bibcite{SM:06}{169}
\bibcite{demvsar2006statistical}{170}
\bibcite{GC09}{171}
\bibcite{BF:08}{172}
\bibcite{zimmerman1997teacher}{173}
