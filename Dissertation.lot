\contentsline {table}{\numberline {2.1}{\ignorespaces The distribution of the roof styles used in the experiments.}}{20}
\contentsline {table}{\numberline {2.2}{\ignorespaces Average endpoint errors (EPE) over occluded (OCC) and non-occluded areas (NOC) of our networks compard to peer methods and the proposed method on different datasets. }}{41}
\contentsline {table}{\numberline {2.3}{\ignorespaces Experiment setups to show predicted optical flows using the proposed approach could improve optical flow predict when combining with data with ground truth optical flow labels.}}{43}
\contentsline {table}{\numberline {2.4}{\ignorespaces Experiment results.}}{44}
\contentsline {table}{\numberline {3.1}{\ignorespaces Precision and recall of hip (HIP), gable (GBL), flat (FLT) and half hip (HHP) styles classified by Random Forest is shown in above table. I use red font to show highest value among results.}}{56}
\contentsline {table}{\numberline {3.2}{\ignorespaces The comparison of accuracies between HoG (HOG), Shape Context (SC), HoR (HOR) and their combinations by running Random Forest approach.}}{58}
\contentsline {table}{\numberline {3.3}{\ignorespaces Accuracy results.}}{67}
\contentsline {table}{\numberline {3.4}{\ignorespaces Results of alignment using global constraint. The number on the right side of the arrows show the results obtained by alignment using the global constraint.}}{68}
\contentsline {table}{\numberline {3.5}{\ignorespaces Precision and recall of Gaussian Mixture Model(GMM), K-Means(KM) and my approach are shown in above table. I use red font to show highest value among results obtained by these three approaches.}}{75}
\contentsline {table}{\numberline {4.1}{\ignorespaces F1-score of roof style classification using reconstructed images (in CNN) and encoded image features (in SVM). Second column shows the data used to train the autoencoder in the first column. In classification, Real+\textit {Syn II} are used in the training of CNN and SVM. ; $\textit {Syn I}+\text {Real}$ means that I use concatenation of the Syn I and real images as the input for the corresponding autoencoders.}}{88}
\contentsline {table}{\numberline {4.2}{\ignorespaces F1-score of handwritten digit recognition.}}{88}
\contentsline {table}{\numberline {4.3}{\ignorespaces F1-score of roof style classification by classifier (CNN and SVM) using different set of data reconstructed of encoded using the proposed MCAE.}}{91}
\contentsline {table}{\numberline {4.4}{\ignorespaces F1-score of handwritten digit recognition.}}{91}
\contentsline {table}{\numberline {5.1}{\ignorespaces Summary of the datasets used in our experiments, where S\#, F\#, and R stand for the number of samples, the number of features, and imbalance ratio (defined as \#minority$/$\#majority).}}{103}
\contentsline {table}{\numberline {5.2}{\ignorespaces A summary of performance measures for the majority and minority classes for all compared methods and artificial datasets.}}{105}
\contentsline {table}{\numberline {5.3}{\ignorespaces A summary of AUC, $Precision$, $Recall$, $\mathop {F\mhyphen score}$ and $\mathop {G\mhyphen score}$ of all competitors for the majority and minority classes produced by 6 classifiers on the artificial datasets.}}{108}
\contentsline {table}{\numberline {5.4}{\ignorespaces A summary of AUC of 8 oversampling algorithms over all 30 datasets used in our evaluation. The AUC is averaged over all 6 base classifiers used in the evaluation. It could be seen from above table that CGMOS achieves best AUC measures for 24 datasets out of 30. By average, the AUC of CGMOS is at least 2 percent higher than all other competitors.}}{117}
\contentsline {table}{\numberline {5.5}{\ignorespaces A summary of $p$-values of statistical significant tests of classification results using CGMOS against each of all the other competitors. }}{118}
